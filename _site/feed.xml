<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-24T22:10:39+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jotter</title><subtitle>I write notes about various things I learn about, hoping to organize my thoughts and share them with others.</subtitle><entry><title type="html">Paper review: Predicting Deeper into the Future of Semantic Segmentation</title><link href="http://localhost:4000/review/2020/05/24/predsem.html" rel="alternate" type="text/html" title="Paper review: Predicting Deeper into the Future of Semantic Segmentation" /><published>2020-05-24T00:02:44+02:00</published><updated>2020-05-24T00:02:44+02:00</updated><id>http://localhost:4000/review/2020/05/24/predsem</id><content type="html" xml:base="http://localhost:4000/review/2020/05/24/predsem.html">&lt;h1 id=&quot;paper-review---predicting-deeper-into-the-future-of-semantic-segmentation&quot;&gt;Paper review - Predicting Deeper into the Future of Semantic Segmentation&lt;/h1&gt;

&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;In robotics it is crucial to correctly interpret the agent’s environment for good decision making. Besides the semantics and geometry of a scene, it is important to understand the scene dynamics. The here analyzed paper &lt;em&gt;Predicting Deeper into the Future of Semantic Segmentation&lt;/em&gt; [1] focuses on this task and generates segmentation maps for not yet seen video frames based on a given input sequence (see Fig. 1). This blog post discusses the contributions and critique of the limitations of the work described. Additionally the following questions are answered: How the authors worked to overcome or mitigate any limitations and how does the work fit in the wider body of literature? In the end the scientific methodology and experimental results are evaluated and an outlook on the potential use for autonomous driving is given.
&lt;!--more--&gt;&lt;/p&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/predsem_img/title.PNG&quot; alt=&quot;predsem_title&quot; style=&quot;zoom:60%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 1: Semantic-level scene dynamics to predict semantic segmentations of unobserved future frames given several past frames [1].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;main-contributions-of-paper&quot;&gt;Main contributions of paper&lt;/h4&gt;

&lt;h5 id=&quot;introduction-of-the-task-of-predicting-semantic-segmentation-maps-for-future-frames-in-a-video-sequence&quot;&gt;Introduction of the task of predicting semantic segmentation maps for future frames in a video sequence&lt;/h5&gt;

&lt;p&gt;Previous works in video forecasting either focus on predicting raw RGB intensities [2] of future frames or avoid the high dimensional pixel space by focusing on semantic properties. For this different forms of abstractions like directly predicting frame transformations [3], visual representations [4] or feature point trajectories [5] are used. Pauline Luc et al. relax the problem by focusing on ”future high-level scene properties” and therefore introduce the new task of directly predicting semantic segmentation maps. The underlying idea is that the capacity of the model is better allocated by focusing on the dense semantic information of the scene, than on the RGB values, which are not required for many applications.&lt;/p&gt;

&lt;h5 id=&quot;development-of-an-autoregressive-model-which-enables-stable-mid-term-predictions&quot;&gt;Development of an autoregressive model, which enables stable mid-term predictions&lt;/h5&gt;

&lt;p&gt;For single-frame prediction a model is presented, which takes N semantic segmentation maps as input and directly predicts the segmentation for the next timestep t+1. The input and target maps get precomputed using a Dilation10 network [6]. It is shown that the models, which just use segmentations as inputs and targets outperform these, which additionally use or predict RGB values or just focus on the prediction of raw pixel intensities. The architecture of the described S2S model is a multi-scale network like in [2], with two spatial scales. It predicts a coarse global output for a downsampled input map and uses the upsampled output of the first module as well as the correct scaled input frames to refine this prediction in the second module. Instead of using the class values of the semantic segmentation maps as inputs and targets, the final softmax layer’s pre-activations are used as they have shown to contain more information [7] (see Fig. 2).&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/predsem_img/architecture.PNG&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:45%;&quot; class=&quot;center&quot; /&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/predsem_img/batchVSautoregressive.PNG&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:55%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 2: Multi-scale architecture of the S2S model that predicts the semantic segmentation of the next frame given the segmentation maps of the NI previous frames. The autoregressive model (top right) shares parameters over time, in contrast to the batch model (bottom right); dependency links are colored accordingly. [1]&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To predict more than one time step in the future Pauline Luc et al. propose an autoregressive model based on the described architecture (see Fig. 2). The predicted output for timestep t gets used as input in the next iteration and the next frames are predicted in an iterative manner. It is shown that this approach, which exploits the recurrent structure of the problem, outperforms the variant, which based on the input at t predicts a batch of all required future frames. For training the network, losses with different properties are evaluated:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L1-loss: Matches pixel/pre-activation predictions&lt;/li&gt;
  &lt;li&gt;Gradient difference loss (GDL): Matches regional differences (e.g. on contours)&lt;/li&gt;
  &lt;li&gt;Adversial loss based on EM distance: Allows different turns of events as they do not need to match the oracle but just require to fool the discriminator&lt;/li&gt;
  &lt;li&gt;Autoregressive fine-tuning: Backpropagation through time to account for error propagation in autoregressive setup&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is shown that the best variant for mid-term prediction is trained with L1, GDL and autoregressive fine-tuning on the softmax pre-activations. The final model is able to outperform the two baselines (I) copying the input frame to output and II) a warped form of the input frame based on estimated optical flow) for short-term and mid-term predictions (up to 0.5 s). Long term predictions (up to 10 s) show much worse results than I).&lt;/p&gt;

&lt;h4 id=&quot;classification-in-the-broader-spectrum-of-literature&quot;&gt;Classification in the broader spectrum of literature&lt;/h4&gt;

&lt;p&gt;Predicting scene dynamics is a widely studied topic. As described in II-A) there are several ideas to reduce the complexity by concentrating on high-level scene properties instead of modeling the raw pixel intensities. Here we focus on two streams of research, which predict future scenes at a semantically meaningful level: Motion forecasting for objects and segmentations.&lt;/p&gt;

&lt;h5 id=&quot;object-forecasting&quot;&gt;Object forecasting&lt;/h5&gt;

&lt;p&gt;Approaches, especially in autonomous driving, mainly use a three step approach: Detection, tracking and motion forecasting. Tracking systems identify tracks of existing objects in physically meaningful 3D world coordinates, like birds eye view, by exploiting depth information with their sensor setup [8], or in the 2D image plane by using position and object scale [9]. Herefore often extended Kalman filters are used to update the non-linear motion models based on the past measurements, which use the results of the respective object detectors [8]. Based on the identified tracks, the future trajectories are then extrapolated. This can be done by using linear models, which use the motion model of the Kalman filter, behavioral models [10], interacting Gaussian processes [11] or LSTMs [12]. Instead of learning these modules independently, approaches like Fast and Furious [13] or IntentNet [14] try to directly exploit spatio-temporal 3D information, which enables endto-end learning and a better propagation of uncertainties.&lt;/p&gt;

&lt;h5 id=&quot;segmentation-forecasting&quot;&gt;Segmentation forecasting&lt;/h5&gt;

&lt;p&gt;The approach analysed in this report [1] falls within this research area, where the goal is not to focus on selected objects but moreover predicting the whole semantic evolution of the scene. This enables a more holistic form of visual scene understanding. Several publications build upon the here done ground work. P. Luc et al. [15] modify the problem statement in their follow up work by predicting future instance segmentations to overcome the problem that semantic segmentations do not account for individual objects. They realise their approach by predicting the highest level features of the used Mask R-CNN for future frames. These two works get fused in [16] by encoding instance and semantic segmentation information in a single representation using distance maps. Other ideas to improve the prediction of future semantic segmentations include jointly predicting scene parsing and optical flow for future frames [17]. Due to the synergy of both tasks this approach is able to reveal significant better results. Further improvements got achieved by using a bidirectional ConvLSTM, which helps to encode the temporal relation of the input frames [18]. An other extension is proposed in [19], where the authors try to capture epistemic and aleatoric uncertainty in the predicted segmentations by at the same time encouraging diversity in the predicted multi-modal futures. They introduce a new Bayesian inference framework, which goes beyond the often used log-likelihood estimate, which enforces models to predict the mean of possible scenarios.&lt;/p&gt;

&lt;h4 id=&quot;discussion&quot;&gt;Discussion&lt;/h4&gt;

&lt;p&gt;This section has the goal to critically discuss limitations of the presented work and show how the authors mitigated some of them. The focus here is to show, which properties are required to extract meaningful representations of visual scenes, which enable predictions of the future and build a strong basis for decision making, and if the presented model is able to capture them.&lt;/p&gt;

&lt;h5 id=&quot;limitations-of-the-work&quot;&gt;Limitations of the work&lt;/h5&gt;

&lt;p&gt;One of the key limitations of the proposed work is that the system is not explicitly aware of the 3D geometry of the scene as the here proposed input is a projection into 2D space. Furthermore no notion of instances is available. These restrictions have direct impact on the performance and can be seen as reason for the reported poor performance of the model, when handling occlusions. Another weakness is the inability to deal with long term dependencies. The cause for this can be found in the architecture as the model takes four input frames and is therefore limited exploiting past information. Furthermore no pass-through of hidden information like in recurrent networks is possible. The inability to predict multi-modal futures for constant inputs and estimating the uncertainty of events is another restriction. This could have been mitigated by using the presented adversial loss during training and choosing a sampling scheme like in Bayesian inference or using a GAN or VAE like model to get a meaningful probability distribution. The performance of the prediction is also constrained as no ground truth data is available and the targets propagate the errors of the preceding segmentation network.&lt;/p&gt;

&lt;h5 id=&quot;mitigated-limitations&quot;&gt;Mitigated limitations&lt;/h5&gt;

&lt;p&gt;The use of the semantic segmentation model as oracle reduces on the other hand the dependency on costly dense video annotations, which makes it easy to scale the approach to large datasets. Another mitigated limitation is the extension of the task from forecasting trajectories of selected objects to semantic segmentation, which seems to be a more complete form of visual scene understanding. Additionally the autoregressive architecture enables an unlimited view into the future as the model can theoretically predict scenes of arbitrary length.&lt;/p&gt;

&lt;h5 id=&quot;evaluation-of-the-scientific-methodology-and-experimental-results&quot;&gt;Evaluation of the scientific methodology and experimental results&lt;/h5&gt;

&lt;p&gt;The paper impresses by its constant bottom up approach, where initially several options are presented, then compared and in the end a comprehensible decision is made. Examples are the comparisons of the different model architectures X2X, XS2X, XS2S, XS2XS and S2S as well as the decision for the autoregressive and against the batch approach to predict semantic segmentation maps for multiple future time steps. Even though the prediction of semantic segmentation maps was a novel task at the time of publication, it would be interesting to see how other video forecasting approaches like [2] would have performed in comparison to the presented X2X models. On a side note it is nice to see that the used models were published for reproducibility, unfortunately without the training code. The presented results show limitations regarding accuracy and the inability to predict multi-modal futures as described in IVB, which can be seen in particular in the averaged predictions for long-term scenarios (see Fig. 3).&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/predsem_img/eval.PNG&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:60%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 3: Last input segmentation, and ground truth segmentations at 1, 4, 7, and 10 seconds into the future (top row), and corresponding predictions of the autoregressive S2S model trained with fine-tuning (bottom row) [1].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;conclusion-and-outlook-on-application-in-autonomous-driving&quot;&gt;Conclusion and outlook on application in autonomous driving&lt;/h4&gt;

&lt;p&gt;I personally enjoyed reading the paper as well as seeing the development of the idea in the follow up works towards a more holistic form of visual scene understanding. It appears interesting to me how the presented approach can be scaled to be robust for various scenes and edge cases by being simultaneously real time capable. On the other hand, I am curious to see how it will be possible to incorporate 3D geometric information, long term temporal relationships and an awareness of uncertainty for the multi-modal future and how these representations can be leveraged in real life robotic applications.&lt;/p&gt;

&lt;p&gt;Wayve demonstrated in their published work &lt;em&gt;Probabilistic Future Prediction for Video Scene Understanding&lt;/em&gt; [20], that learning temporal representations improve their driving policy for their reinforcement learning based end-to-end approach. Here I focus on the aforementioned addressed limitations, and how they are tried to be mitigated in there paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Geometry and instance awareness:&lt;/strong&gt; The additional auxiliary task of depth estimation introduces information about the 3D geometry of the scene. Limitations, which could not have been mitigated are in my opinion that the system still works with a 2D projection of the scene and also no explicit knowledge of instances is available.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Long term dependencies:&lt;/strong&gt; The use of the spatio-temporal module, which uses 3D convolutions to encode temporal, spatial and global context, seems to be a more sophisticated choice than the in of P. Luc et al. used approach of 2D convolutions for stacked input frames. Still the framework has limitations to make use of long term relationships as the number of input frames is fixed and no pass-through of information possible. The temporal encoding gets additionally enforced by the auxiliary task of flow estimation in the perception module.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-modal futures:&lt;/strong&gt; Sampling future predictions from a distribution, which is conditioned on the past context and trained by minimizing the distance to the future distribution of observed sequences, shows visually convincing and plausible results. This mitigates the key limitation of the approach presented by P. Luc et al., which was not able to capture any form of multimodality and averaged possible futures.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Estimation of uncertainties:&lt;/strong&gt; Another extension is the ability to estimate uncertainties about the predicted future scenes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limitation of performance caused by use of precomputed semantic segmentation maps:&lt;/strong&gt; In contrast to P. Luc et al. the here presented framework is using raw RGB images as input and is therefore just depending on the precomputed semantic segmentation for the training targets, which decreases the influence of potential error propagation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability and real-time capability:&lt;/strong&gt; The here proposed work also shows that the idea of predicting future semantic segmentation maps can be scaled by at the same time being real-time capable and that the learned representations are able to improve real-world driving policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;p&gt;[1] N. Neverova, P. Luc, C. Couprie, J. J. Verbeek, and Y. LeCun, “Predicting deeper into the future of semantic segmentation,” CoRR, vol. abs/1703.07684, 2017.&lt;/p&gt;

&lt;p&gt;[2] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video prediction beyond mean square error,” 2015.&lt;/p&gt;

&lt;p&gt;[3] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for physical interaction through video prediction,” 2016.&lt;/p&gt;

&lt;p&gt;[4] C. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating visual representations from unlabeled video,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 98–106, June 2016.&lt;/p&gt;

&lt;p&gt;[5] J. Walker, C. Doersch, A. Gupta, and M. Hebert, “An uncertain future: Forecasting from static images using variational autoencoders,” CoRR, vol. abs/1606.07873, 2016.&lt;/p&gt;

&lt;p&gt;[6] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,” 2015.&lt;/p&gt;

&lt;p&gt;[7] L. J. Ba and R. Caruana, “Do deep nets really need to be deep?,” CoRR, vol. abs/1312.6184, 2013.&lt;/p&gt;

&lt;p&gt;[8] A. Ess, K. Schindler, B. Leibe, and L. V. Gool, “Object detection and tracking for autonomous navigation in dynamic environments,” The International Journal of Robotics Research, vol. 29, pp. 1707 – 1725, 2010.&lt;/p&gt;

&lt;p&gt;[9] L. Zhang, Y. Li, and R. Nevatia, “Global data association for multiobject tracking using network flows,” 06 2008.&lt;/p&gt;

&lt;p&gt;[10] K. Yamaguchi, A. C. Berg, L. E. Ortiz, and T. L. Berg, “Who are you with and where are you going?,” in CVPR 2011, pp. 1345–1352, June 2011.&lt;/p&gt;

&lt;p&gt;[11] P. Trautman, J. Ma, R. M. Murray, and A. Krause, “Robot navigation in dense human crowds: Statistical models and experimental studies of human–robot cooperation,” The International Journal of Robotics Research, vol. 34, pp. 335 – 356, 2015.&lt;/p&gt;

&lt;p&gt;[12] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese, “Social lstm: Human trajectory prediction in crowded spaces,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 961–971, June 2016.&lt;/p&gt;

&lt;p&gt;[13] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time endto-end 3d detection, tracking and motion forecasting with a single convolutional net,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3569–3577, June 2018.&lt;/p&gt;

&lt;p&gt;[14] S. Casas, W. Luo, and R. Urtasun, “Intentnet: Learning to predict intention from raw sensor data,” in Proceedings of The 2nd Conference on Robot Learning (A. Billard, A. Dragan, J. Peters, and J. Morimoto, eds.), vol. 87 of Proceedings of Machine Learning Research, pp. 947– 956, PMLR, 29–31 Oct 2018.&lt;/p&gt;

&lt;p&gt;[15] P. Luc, C. Couprie, Y. LeCun, and J. Verbeek, “Predicting future instance segmentations by forecasting convolutional features,” CoRR, vol. abs/1803.11496, 2018.&lt;/p&gt;

&lt;p&gt;[16] C. Couprie, P. Luc, and J. Verbeek, “Joint Future Semantic and Instance Segmentation Prediction,” in ECCV Workshop on Anticipating Human Behavior, vol. 11131 of Lecture Notes in Computer Science, (Munich, Germany), pp. 154–168, Springer, Sept. 2018.&lt;/p&gt;

&lt;p&gt;[17] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng, and S. Yan, “Predicting scene parsing and motion dynamics in the future,” 2017.&lt;/p&gt;

&lt;p&gt;[18] S. S. Nabavi, M. Rochan, and Y. Wang, “Future semantic segmentation with convolutional LSTM,” CoRR, vol. abs/1807.07946, 2018.&lt;/p&gt;

&lt;p&gt;[19] A. Bhattacharyya, M. Fritz, and B. Schiele, “Bayesian prediction of future street scenes using synthetic likelihoods,” CoRR, vol. abs/1810.00746, 2018.&lt;/p&gt;

&lt;p&gt;[20] A. Hu, F. Cotter, N. Mohan, C. Gurau, and A. Kendall, “Probabilistic future prediction for video scene understanding,” arXiv:2003.06409, 2020.&lt;/p&gt;</content><author><name>Maximilian Bömer</name></author><summary type="html">Paper review - Predicting Deeper into the Future of Semantic Segmentation Introduction In robotics it is crucial to correctly interpret the agent’s environment for good decision making. Besides the semantics and geometry of a scene, it is important to understand the scene dynamics. The here analyzed paper Predicting Deeper into the Future of Semantic Segmentation [1] focuses on this task and generates segmentation maps for not yet seen video frames based on a given input sequence (see Fig. 1). This blog post discusses the contributions and critique of the limitations of the work described. Additionally the following questions are answered: How the authors worked to overcome or mitigate any limitations and how does the work fit in the wider body of literature? In the end the scientific methodology and experimental results are evaluated and an outlook on the potential use for autonomous driving is given.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.researchgate.net/profile/Jakob_Verbeek/publication/315514643/figure/fig2/AS:667791610871819@1536225335330/Given-the-semantic-segmentation-of-past-frames-obtained-by-a-state-of-the-art-model-our.png" /><media:content medium="image" url="https://www.researchgate.net/profile/Jakob_Verbeek/publication/315514643/figure/fig2/AS:667791610871819@1536225335330/Given-the-semantic-segmentation-of-past-frames-obtained-by-a-state-of-the-art-model-our.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dense predictions: Review of selected publications in semantic and instance segmentation</title><link href="http://localhost:4000/review/2020/03/19/semseg.html" rel="alternate" type="text/html" title="Dense predictions: Review of selected publications in semantic and instance segmentation" /><published>2020-03-19T23:02:44+01:00</published><updated>2020-03-19T23:02:44+01:00</updated><id>http://localhost:4000/review/2020/03/19/semseg</id><content type="html" xml:base="http://localhost:4000/review/2020/03/19/semseg.html">&lt;h2 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h2&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Semantic segmentation is one of the core problems of computer vision. It describes the task of associating each pixel of a input image with a semantic class and can be applied for various problems, where visual scene understanding is important. I had my first contact with semantic segmentation in 2017, when I started my master thesis, in which I developed a system to detect and classify road damages and obstacles based on dash cam images. I used semantic segmentation to extract the street surface and then identify anomalies with classical computer vision approaches. Later I applied the technique in both image anonymization and scene parsing for autonomous driving and also worked on a project for video object segmentation. The different tasks had different requirements and it is always a challenge to balance computational efficiency and accuracy. This post tries to give an by far not complete overview about the field and explain some of the key ideas. Additionally an outlook on instance segmentation is given. Interesting applications to other forms of data like point clouds or videos are planned for an upcoming blog post.
&lt;/div&gt;
&lt;!--more--&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/semseg.png&quot; alt=&quot;SemSeg&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 1: Example for an input output pair in semantic segmentation [1].&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;pre-deep-learning-era&quot;&gt;Pre Deep Learning Era&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
In the time before CNNs were directly providing dense category-level pixel labels, a variety of different approaches existed. These techniques were using different kind of inputs like the pixel color, histogram of oriented gradients, SIFT, SURF or other forms of features. In this section I want to focus on &lt;b&gt;Conditional Random Fields&lt;/b&gt; based techniques, which are still used today and now mostly get applied as a postprocessing step to refine deep learning based results.

Conditional random fields are a variant of Markov networks, which are designed to solve task-specific predictions. This are tasks, where we have a set of input variables X (here: image/super pixels) and a set of target variables, that we're trying to predict Y (here: class for every pixel). The problem is defined as a classification of an element, which depends on the labels of the neighboring elements of the observation. Therefore the complex dependencies between the features can be ignored and instead of modeling the joint probability distribution &lt;i&gt;p(y,x)&lt;/i&gt;, the solution to this problem can be defined as a discriminative approach, where we directly model the conditional distribution &lt;i&gt;p(y|x)&lt;/i&gt;.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/formula_CRF.png&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:75%;&quot; class=&quot;center&quot; /&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/CRF_2.png&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/crf_semseg.png&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:80%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 2: Semantic segmentation using superpixels and CRFs [1].&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
The semantic segmentation process is divided into different subtasks. Normally the image is first clustered into superpixels using features like appearance similarity, spatial distance and contour information to improve computational efficiency and robustness. The second step is then to evaluate the likelihood of a superpixel belonging to a given object class. Therefore models like Adaboost or random forest classifiers, SVMs or CNNs are used. After that a conditional random field (CRF) is applied to model the dependencies between the class probabilities of the different superpixels.

The final predictions can then be calculated by estimating the Maximum Likelihood parameters (see Figure 2). The first term represents the impact of the local evidence around &lt;i&gt;x_i&lt;/i&gt; to label &lt;i&gt;y_i&lt;/i&gt;, while the second one encourages neighboring labels to be similar, in a manner that depends on the difference in pixel intensity and therefore on the choice of &lt;i&gt;g&lt;/i&gt;. &lt;i&gt;Beta&lt;/i&gt;, &lt;i&gt;theta&lt;/i&gt; and &lt;i&gt;lambda&lt;/i&gt; are the parameters of the model, which are learned to optimize the performance based on a training set.
&lt;/div&gt;
&lt;h3 id=&quot;fcn---fully-convolutional-networks-for-semantic-segmentation&quot;&gt;FCN - Fully Convolutional Networks for Semantic Segmentation&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Deep learning approaches directly map the input to the dense output masks and the network parameters are optimized with a pixel-wise cross entropy loss. The general semantic segmentation architecture is an encoder-decoder network. The encoder extracts the important information of the raw RGB data, while the task of the decoder is to project the learned features onto the pixel space to get a dense classification.

Jonathan Long et al. [2] were the first, who applied the idea of fully convolutional networks to semantic segmentation. The main idea is here the &quot;&lt;b&gt;convolutionalization&lt;/b&gt;&quot;, where all fully connected layers get replaced by the same number of 1x1 convolutional filters. The resulting networks are therefore just consisting of convolutional, pooling and upsampling layers and can take images of arbitrary size as input plus are end-to-end trainable using the per-pixel softmax loss.  This idea enables additionally the use of pre-trained encoders from large-scale classification tasks like ImageNet (e.g. VGG-16) .
To mitigate the problem of losing local context in the created output maps, &lt;b&gt;skip connections&lt;/b&gt; were introduced (see Fig. 3). These fuse the information from different feature maps of the encoder by summation and enable a synthesis of low-level information as well as global context.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/fcn_architecture.png&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 3: Architectures of FCN-32, FCN-16 and FCN-8 using different form of skip connections to increase precision by the use of local information [2].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;u-net&quot;&gt;U-Net&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Bio-medical applications require high precision. U-Net, which was published by Ronneberger et al. [3] can be seen as an extension of FCN to work with a small amount of training samples by at the same time having high precision. The main differences are summarized below.

As visible in Fig. 4 U-Net has &lt;b&gt;multiple upsampling layers with learnable weight filters&lt;/b&gt;, which differs from the original FCN implementation, which only consists of just one decoder layer with bilinear interpolation. The increased depth and the use of a high number of learnable feature channels allows to propagate more context information to the higher resolution layers and results in a higher precision and mitigates the limitation of local accuracy caused by simple bilinear interpolation.

The second advantage of the symmetric architecture is the possibility to transfer information from each encoding step to the respective decoding stage. In contrast to FCN, &lt;b&gt;skip connections use concatenation of feature maps instead of summation&lt;/b&gt;. This helps aggregating information at multiple scales and fusing it in a learned way by the following convolutions. The ability to learn from a small number of examples is due to the extensive data augmentations, which get used. Besides shift, rotation and illumination changes, the authors use random elastic deformations for a robust training process.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/unet_architecture.jpg&quot; alt=&quot;dilated_conv&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 4: Architecture of U-Net [3]&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;multi-scale-context-aggregation-by-dilated-convolutions&quot;&gt;Multi-scale context aggregation by dilated convolutions&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Semantic segmentation is a difficult task. The challenge is to reach on the one side pixel-level accuracy and reason at the same time about the global context. Fisher Yu from Princeton University and Vladlen Koltun from Intel Labs used a convolutional network module that combines both: &lt;b&gt;Dilated convolutions&lt;/b&gt; [4]. The idea of the approach is to not rely on pooling or subsampling operations and therefore prevents the explicit loss of information and hence reduced resolution, while at the same time expanding the receptive field of view of the model.

The dilated convolution operator (see Fig. 5), also known as atrous convolution and known since the 80s, applies the same filter with different dilation factors l. The used &lt;b&gt;basic context module&lt;/b&gt; consists of seven 3x3 convolutions with different dilation factors, where each filter is applied to all channels of the input tensor, and afterwards truncated point-wise.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
    &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/formula_dilated_conv.png&quot; alt=&quot;standard_conv&quot; style=&quot;zoom:80%;&quot; class=&quot;center&quot; /&gt;
&lt;/figure&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/standard_conv.gif&quot; alt=&quot;standard_conv&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/dilated_conv.gif&quot; alt=&quot;dilated_conv&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 5: Standard (l=1) vs. dilated convolution (l=2).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Besides the new technique to aggregate context, Yu et al. also presented a novel &lt;b&gt;front end module&lt;/b&gt;. Like in the FCN a VGG-16 network gets adapted for dense predictions (see Fig. 2). Instead of using pooling and striding layers, which were formerly introduced for reducing the spatial dimensions for classification tasks, these operations get replaced by dilated convolutions. This simplified model showed more accurate results than pior architectures and emphasizes the argument of the authors, that dedicated architectures for dense prediction lead to increased performance.
&lt;/div&gt;
&lt;h3 id=&quot;deeplab----semantic-image-segmentation-with-deep-convolutional-nets-atrous-convolution-and-fully-connected-crfs&quot;&gt;Deeplab -  Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Chen et al. [5] developed in their first Deeplab paper three interesting techniques, which reached a new state-of-art at the PASCAL VOC-2012 [8] semantic image segmentation challenge. One of the ideas is the use &lt;b&gt;atrous convolutions&lt;/b&gt;, which is a synonym for dilated convolutions, and were described above.

The second new technique is the &lt;b&gt;atrous spatial pyramid pooling&lt;/b&gt; module (see Fig. 6). It uses multiple parallel atrous convolutions with different sampling rates. The features are extracted for each of the sampling rates and are then post-processed in separate branches and finally fused together. This module helps by explicitly accounting for object scale and improves the ability to handle both large and small objects.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/atrous_pp.png&quot; alt=&quot;standard_conv&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 6: Atrous spatial pyramid pooling module&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;div style=&quot;text-align: justify&quot;&gt;
A general trade-off for deep CNNs is that the stacking of pooling and convolutional layers does improve the aggregation of global context, which is important for the scene classification, but also has a negative effect on the local accuracy and therefore only yields smooth responses (see Fig. 7). &lt;b&gt;Fully connected Conditional Random Fields&lt;/b&gt; were therefore introduced as a postprocessing step to recover accurate boundaries. The energy function incorporates on the one hand the unary potential, which depends on the assignment probability at pixel &lt;i&gt;i&lt;/i&gt;, and on the other side also the pairwise potential. Here nodes with distinct labels are penalized based on their pixel position and RGB color.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/energy_function_CRF.png&quot; alt=&quot;standard_conv&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/CRF_iterations.png&quot; alt=&quot;standard_conv&quot; style=&quot;zoom:80%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 7: CNN output and belief maps after mean field iterations.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;div style=&quot;text-align: justify&quot;&gt;
In the later versions of Deeplab different ideas got added. Atrous depth-wise separable convolutions were introduced to increase computational efficiency and additionally slightly improved the performance. In Deeplabv3+ all max pooling operations got replaced by depth-wise separable convolutions and additionally low-level features were used to recover local accuracy.
&lt;/div&gt;
&lt;h3 id=&quot;pspnet---pyramid-scene-parsing-network&quot;&gt;PSPNet - Pyramid Scene Parsing Network&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
To associate each input pixel with the correct class, global information can be helpful. If you see for example an object on a lake and you can not decide by its appearance, if it is a ship or a car. The context helps as cars are rarely swimming. PSPNet [6] addresses this issue and therefore introduced the &lt;b&gt;pyramid pooling module&lt;/b&gt;. 

The pyramid pooling module combines in the original paper feature maps from four different scales. Coarse global as well as regional information get extracted, the dimensions reduced by 1x1 convolutions and afterwards upsampled to the spatial resolution of the input feature map. In the end the different representations as well as the input tensor get concatenated and are then used as a base for the decoder, which creates the final prediction. This architectural prior helps to combine low- as well as high-level information.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/psp_module.png&quot; alt=&quot;fcn_architecture&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 8: Pyramid parsing module exploiting different sub-region representations [6].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;erfnet---efficient-residual-factorized-convnet-for-real-time-semantic-segmentation&quot;&gt;ERFNet - Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Robotic applications like autonomous driving have challenging real time requirements. Eduardo Romera et al. [7] addressed this problem in their work and tried to find a good compromise between accuracy and computational efficiency (83 FPS on a single Titan X). Their key idea is the use of &lt;b&gt;factorized residual layers&lt;/b&gt;.

The original ResNet paper is proposing the non-bottleneck design (a) with two 3x3 convolutions and the bottleneck design (b) with one 3x3 convolution framed by 1x1 convolutions (see Fig. 9). The bottleneck version is computationally more efficient as the size of the input tensor can be reduced by the cheap 1x1 convolutions and the computationally expensive 3x3 convolutions have to be applied to a smaller tensor. This increases efficiency but also has negative effects on the accuracy.

Romera et al. propose a new design based on spatial separable convolutions. They decompose the 3x3 kernels from the original non-bottleneck design by two 1-dimensional convolutions 1x3 and 3x1. The different number of weights for a single input channel and one filter shows the advantage:  a) 2 * 3 * 3 = 18 b) 4 * 3 * 1 = 12. This also translates in a reduced number of multiplications and therefore more generally speaking in lower computational cost. Additionally, the 1D convs remove redundancies (shared for each 2D combination) and also the additional non-linearities can improve the training success.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/non_bottleneck_1d.png&quot; alt=&quot;non_bottleneck_1d&quot; style=&quot;zoom:40%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 9: Comparison of variants of residual layers [7].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;instance-segmentation&quot;&gt;Instance segmentation&lt;/h2&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
While semantic segmentation tries to solve the task of predicting for each input pixel the associated semantic class, the goal of instance segmentation can be divided in two subgoals: 1) Detect all defined objects 2) Correctly segment each of the instances.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/instance_segmentation.png&quot; alt=&quot;non_bottleneck_1d&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 10: Difference between semantic and instance segmentation [8].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;mask-r-cnn&quot;&gt;Mask R-CNN&lt;/h3&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
Mask R-CNN [9] builds upon the work of Faster R-CNN [10], which gets therefore briefly summarized first. Before the time of Faster R-CNN region proposals were created by algorithms like selective search. For each of the candidate object locations the relevant features got pooled from the feature map, which were normally generated by CNN encoders. This features were then used for the following classification and bounding box regression step. The identification of region proposals with selective search is computationally expensive and therefore also time intensive. The key idea of Faster R-CNN is the introduction of a &lt;b&gt;Region Proposal Network&lt;/b&gt; (RPN), which predicts objectness scores and bounds for each position of the feature map (see Fig. 9). As the feature map gets shared with the detection network, the region proposals add hardly additional computational cost. The RPN uses reference boxes/anchors to improve the region proposals. For each of the sliding-window locations the objectness score gets computed for three different aspect ratios and scales. For each object proposals of the RPN the &lt;b&gt;RoI pooling layer&lt;/b&gt; extracts a fixed-length feature vector from the feature map, which is fed into a sequence of fully connected layers. To get a fixed size output from ROI pooling, the cropped feature map is divided into bins and from each bin the maximum or average value is taken. This process is called quantization. Finally the header network is splitted into two output layers: 1) Produces softmax probability over K object classes plus a background class, 2) Outputs refined bounding-box positions.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/faster_r-cnn.png&quot; alt=&quot;faster_r-cnn&quot; style=&quot;zoom:30%;&quot; class=&quot;center&quot; /&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/faster_r-cnn2.png&quot; alt=&quot;faster_r-cnn&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 9: Architecture of Faster R-CNN [10].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;div style=&quot;text-align: justify&quot;&gt;
Mask R-CNN adds a third branch, that outputs K (number of classes) masks for each RoI (see Fig. 10). Therefore the identified small feature maps are fed into a FCN, which predicts the pixel-wise masks and in the end the final k-th mask is chosen based on the prediction of the classification branch. As this task requires preserved per-pixel spatial correspondence, the &lt;b&gt;RoIAlign&lt;/b&gt; layer was introduced. In RoIPool, we loose information at two points: When we crop the feature map and just take the integer part and when the cropped part is divided into a grid with fixed size. RoIAlign uses therefore float numbers to crop the feature map and bilinear interpolation to compute the grid values to mitigate this information loss.

Furthermore the authors showed, that additional branches for example human keypoint detection could be easily integrated and jointly trained.
&lt;/div&gt;
&lt;figure align=&quot;center&quot;&gt;
  &lt;img img=&quot;&quot; src=&quot;/assets/semseg_img/MaskRCNN.png&quot; alt=&quot;faster_r-cnn&quot; style=&quot;zoom:50%;&quot; class=&quot;center&quot; /&gt;
  &lt;figcaption&gt;Fig. 10: Branches of Mask R-CNN [9].&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;

&lt;p&gt;[1]:  S. Nowozin and C. H. Lampert. Structured learning and prediction in computer vision. Foundations and Trends in Computer Graphics and Vision, 6:185–365, 2011&lt;/p&gt;

&lt;p&gt;[2]: Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR., 2015&lt;/p&gt;

&lt;p&gt;[3]:  O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234–241. Springer, 2015&lt;/p&gt;

&lt;p&gt;[4]: F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016&lt;/p&gt;

&lt;p&gt;[5]: L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv:1606.00915, 2016&lt;/p&gt;

&lt;p&gt;[6]: Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: CVPR, 2017&lt;/p&gt;

&lt;p&gt;[7]: Romera, E., Alvarez, J.M., Bergasa, L.M., Arroyo, R.: Erfnet: Efficient residual factorized convnet for real-time semantic segmentation. IEEE Transactions on Intelligent Transportation Systems, 2018&lt;/p&gt;

&lt;p&gt;[8]: M.Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman, The PASCAL visual object classes VOC dataset and challenge, 2009&lt;/p&gt;

&lt;p&gt;[9]:  K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask R-CNN. In ICCV, 2017&lt;/p&gt;

&lt;p&gt;[10]: S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015&lt;/p&gt;</content><author><name>Maximilian Bömer</name></author><summary type="html">Semantic Segmentation Semantic segmentation is one of the core problems of computer vision. It describes the task of associating each pixel of a input image with a semantic class and can be applied for various problems, where visual scene understanding is important. I had my first contact with semantic segmentation in 2017, when I started my master thesis, in which I developed a system to detect and classify road damages and obstacles based on dash cam images. I used semantic segmentation to extract the street surface and then identify anomalies with classical computer vision approaches. Later I applied the technique in both image anonymization and scene parsing for autonomous driving and also worked on a project for video object segmentation. The different tasks had different requirements and it is always a challenge to balance computational efficiency and accuracy. This post tries to give an by far not complete overview about the field and explain some of the key ideas. Additionally an outlook on instance segmentation is given. Interesting applications to other forms of data like point clouds or videos are planned for an upcoming blog post.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/tuebingen00.png" /><media:content medium="image" url="https://www.cityscapes-dataset.com/wordpress/wp-content/uploads/2015/07/tuebingen00.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>